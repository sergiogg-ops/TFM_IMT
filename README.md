flan-t5, small (60M parametros) -> https://huggingface.co/google/flan-t5-small
flan-t5, base (220M parametros) -> https://huggingface.co/google/flan-t5-base
xlnet -> https://huggingface.co/xlnet-base-cased
bloom, 560M parametros -> https://huggingface.co/bigscience/bloom-560m
llama, small (101M parametros) -> https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA
deberta, base (86M parametros) -> https://huggingface.co/microsoft/deberta-v3-base
deberta, large(304M parametros) -> https://huggingface.co/microsoft/deberta-v3-large
Mamba: investigar si tiene buena pinta y se puede usar para traducción


DATASETS
WMT2016, (inglés, francés, alemán, ruso, checo, finés, rumano) -> https://paperswithcode.com/dataset/wmt-2016
Europarl (21 lenguas europeas) -> https://paperswithcode.com/dataset/europarl
FLORES-101, (101 idiomas) -> https://paperswithcode.com/dataset/flores-101
OPUS-100, (100 idiomas) -> https://paperswithcode.com/dataset/opus-100
MTNT, (frances <-> japones) -> https://paperswithcode.com/dataset/mtnt



MT5, MBart
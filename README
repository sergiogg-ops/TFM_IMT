flan-t5, small (60M parametros) -> https://huggingface.co/google/flan-t5-small

flan-t5, base (220M parametros) -> https://huggingface.co/google/flan-t5-base

xlnet -> https://huggingface.co/xlnet-base-cased

bloom, 560M parametros -> https://huggingface.co/bigscience/bloom-560m

llama, small (101M parametros) -> https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA

tower, sobre Llama → https://huggingface.co/Unbabel/TowerInstruct-7B-v0.1

deberta, base (86M parametros) -> https://huggingface.co/microsoft/deberta-v3-base

deberta, large(304M parametros) -> https://huggingface.co/microsoft/deberta-v3-large

Mamba, fine-tuning 130M -> https://huggingface.co/Q-bert/Mamba-370M

MBart, many-to-one -> https://huggingface.co/facebook/mbart-large-50-many-to-one-mmt

MBart, many-to-many -> https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt

MT5 -> https://huggingface.co/google/mt5-base

M2M, many-to-many -> https://huggingface.co/docs/transformers/model_doc/m2m_100


DATASETS

WMT2016, (inglés, francés, alemán, ruso, checo, finés, rumano) -> https://paperswithcode.com/dataset/wmt-2016

Europarl (21 lenguas europeas) -> https://paperswithcode.com/dataset/europarl

FLORES-101, (101 idiomas) -> https://paperswithcode.com/dataset/flores-101

OPUS-100, (100 idiomas) -> https://paperswithcode.com/dataset/opus-100

MTNT, (frances <-> japones) -> https://paperswithcode.com/dataset/mtnt



https://huggingface.co/learn/nlp-course/chapter7/4
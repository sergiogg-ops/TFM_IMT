MBart, many-to-many (610.879.488 parametros) -> https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt

flan-t5, small (60M parametros) -> https://huggingface.co/google/flan-t5-small

flan-t5, base (220M parametros) -> https://huggingface.co/google/flan-t5-base

M2M, many-to-many (483.905.536 parametros) -> https://huggingface.co/docs/transformers/model_doc/m2m_100

bloom, 560M parametros, PROMT -> https://huggingface.co/bigscience/bloom-560m

tower, sobre Llama, PROMT (6.738.472.960 parametros) → https://huggingface.co/Unbabel/TowerInstruct-7B-v0.1

MT5 , ¿PROMT? (582.401.280 parametros) -> https://huggingface.co/google/mt5-base

llama, small, PROMT (101M parametros) -> https://huggingface.co/BEE-spoke-data/smol_llama-101M-GQA

Mamba, no multilingüe (130M parametros) -> https://huggingface.co/Q-bert/Mamba-370M


DATASETS

WMT2016, (inglés, francés, alemán, ruso, checo, finés, rumano) -> https://paperswithcode.com/dataset/wmt-2016

Europarl (21 lenguas europeas) -> https://paperswithcode.com/dataset/europarl

FLORES-101, (101 idiomas) -> https://paperswithcode.com/dataset/flores-101

OPUS-100, (100 idiomas) -> https://paperswithcode.com/dataset/opus-100

MTNT, (frances <-> japones) -> https://paperswithcode.com/dataset/mtnt



https://huggingface.co/learn/nlp-course/chapter7/4